# model_name: iota-chat-13b-16k
# base_model: conceptofmind/LLongMA-2-13b-16k
# model_family: llama  # if unspecified will use AutoModelForCausalLM/AutoTokenizer
# model_context_window: 16384  # if unspecified will use tokenizer.model_max_length
# data:
#   type: ultrachat
#   dataset: branles14/ultrachat-uncensored_full
#   user_header: "### User:\n"
#   response_header: "### Response:\n"
# lora:
#   r: 8
#   lora_alpha: 32
#   target_modules:  # modules for which to train lora adapters
#   - q_proj
#   - k_proj
#   - v_proj
#   lora_dropout: 0.05
#   bias: none
#   task_type: CAUSAL_LM
# trainer:
#   batch_size: 1
#   gradient_accumulation_steps: 4
#   warmup_steps: 100
#   num_train_epochs: 1
#   learning_rate: 0.0002  # 2e-4
#   logging_steps: 20
# trainer_output_dir: trainer_outputs/
# model_output_dir: models/