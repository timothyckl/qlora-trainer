model_name: phi-2-instruct 
base_model: microsoft/phi-2
model_family: phi
model_context_window: 2048 
data:
  type: alpaca
  dataset: c-s-ale/alpaca-gpt4-data
  instruct_header: "INSTRUCTION:"
  input_header: "INPUT:"
  output_header: "OUTPUT:"
lora:
  r: 16
  lora_alpha: 32
  target_modules:  # modules for which to train lora adapters
  - q_proj
  - k_proj
  - v_proj
  - dense
  - fc1
  - fc2
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
trainer:
  batch_size: 1
  gradient_accumulation_steps: 5
  warmup_steps: 100
  num_train_epochs: 1
  learning_rate: 0.0002  # 2e-4
  logging_steps: 20
trainer_output_dir: trainer_outputs/
model_output_dir: models/
